{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5179bf1b",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Classification with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98ac3c",
   "metadata": {},
   "source": [
    "## PyTorch Basics\n",
    "\n",
    "- Tensor\n",
    "- Autograd\n",
    "- Transforms\n",
    "- Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as functional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch import tensor, eye"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d531d09",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "\n",
    "In mathematics, tensor is a multi-dimensional vector. We generally use \n",
    "[tensor rank](https://mathworld.wolfram.com/TensorRank.html) to describe tensors.\n",
    "\n",
    "- Rank 0 tensor is a scalar\n",
    "- Rank 1 tensor is a vector or array\n",
    "- Rank 2 tensor is a matrix\n",
    "- Rank 3+ tensor is a tensor\n",
    "\n",
    "\n",
    "In Python, we can perform tensor calculation with `numpy`, `pytorch` or `tensorflow`. Since we are doing PyTorch, we can just focus on PyTorch version of tensors. In general PyTorch and TensorFlow tensors are better than `numpy` tensor because the data can be allocated on GPU whereas `numpy` can only go on CPU. However `numpy` is needed for a lot of libraries such as `matplotlib` for plotting data on graph.\n",
    "\n",
    "For example,\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    " 1 & 1 & 1 \\\\ \n",
    " 1 & 1 & 1 \\\\ \n",
    " 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "is defined by\n",
    "\n",
    "```py\n",
    "tensor([\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1]\n",
    "], dtype=int)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d102ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_a = tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]],dtype=int)\n",
    "mat_b = tensor([[2, 2, 2], [2, 2, 2], [2, 2, 2]],dtype=int)\n",
    "identity = eye(3, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c11cd09",
   "metadata": {},
   "source": [
    "$$\n",
    "M_a \\times M_b = \\begin{bmatrix}\n",
    " 1 & 1 & 1 \\\\ \n",
    " 1 & 1 & 1 \\\\ \n",
    " 1 & 1 & 1\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    " 2 & 2 & 2 \\\\ \n",
    " 2 & 2 & 2 \\\\ \n",
    " 2 & 2 & 2\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    " 6 & 6 & 6 \\\\ \n",
    " 6 & 6 & 6 \\\\ \n",
    " 6 & 6 & 6\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_a.matmul(mat_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544cd33",
   "metadata": {},
   "source": [
    "$$\n",
    "M_a \\times M_b = \\begin{bmatrix}\n",
    " 1 & 1 & 1 \\\\ \n",
    " 1 & 1 & 1 \\\\ \n",
    " 1 & 1 & 1\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    " 1 & 0 & 0 \\\\ \n",
    " 0 & 1 & 0 \\\\ \n",
    " 0 & 0 & 1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    " 1 & 1 & 1 \\\\ \n",
    " 1 & 1 & 1 \\\\ \n",
    " 1 & 1 & 1\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de83055",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_a.matmul(identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89508ce9",
   "metadata": {},
   "source": [
    "PyTorch tensors support many operations, please refer to [PyTorch Tensor Tutorial](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) to learn in depth about tensor operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5313b7",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "Autograd is short for automatic gradient. Gradient is a vector of partial derivatives. Autograd performs partial derivatives in runtime on tensor operations. This is the foundational logic for all supervised machine learning algorithms.\n",
    "\n",
    "Consider the following squared error equation, let $L$ denotes the squared loss.\n",
    "\n",
    "$$\n",
    "L(\\vec{x}, \\vec{y}, \\vec{b}, M) = \\Sigma_i\\,(\\vec{y} - M\\vec{x} - \\vec{b})^2\n",
    "$$\n",
    "\n",
    "What is the gradient of $L$? i.e. partial derivatives of $L$ with respect to $x$, $y$, $M$, and $b$? We can compute this quickly by hand.\n",
    "\n",
    "$$\n",
    "\\nabla L = <\\frac{\\partial L}{\\partial x}, \\frac{\\partial L}{\\partial y}, \\frac{\\partial L}{\\partial b}, \\frac{\\partial L}{\\partial M}>\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\Sigma_i\\, 2 * (\\vec{y} - M\\vec{x} - \\vec{b}) * (-M)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y} = \\Sigma_i\\, 2 * (\\vec{y} - M\\vec{x} - \\vec{b})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\Sigma_i\\, 2 * (\\vec{y} - M\\vec{x} - \\vec{b}) * (-1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial M} = \\Sigma_i\\, 2 * (\\vec{y} - M\\vec{x} - \\vec{b}) * (-\\vec{x})\n",
    "$$\n",
    "\n",
    "We can also let PyTorch compute it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b8e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tensor([\n",
    "    [1],\n",
    "    [1]\n",
    "], dtype=float, requires_grad=True)\n",
    "M = tensor([\n",
    "    [2, 2],\n",
    "    [2, 2],\n",
    "], dtype=float, requires_grad=True)\n",
    "b = tensor([\n",
    "    [3],\n",
    "    [3]\n",
    "], dtype=float, requires_grad=True)\n",
    "y = tensor([\n",
    "    [4],\n",
    "    [4]\n",
    "], dtype=float, requires_grad=True)\n",
    "\n",
    "L = torch.sum((y - M.matmul(x) - b)**2)\n",
    "\n",
    "# Show the value of squared loss\n",
    "print(L) \n",
    "\n",
    "# Use autograd to compute partial derivatives for all variables\n",
    "L.backward() \n",
    "print(f\"\\nPartial derivatives of x = {x.grad}\")\n",
    "print(f\"\\nPartial derivatives of y = {y.grad}\")\n",
    "print(f\"\\nPartial derivatives of b = {b.grad}\")\n",
    "print(f\"\\nPartial derivatives of M = {M.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b92aa",
   "metadata": {},
   "source": [
    "Autograd can perform very complicated partial derivative computation, please refer to [Autograd Tutorial](https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f457be",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "\n",
    "When we perform image classification, we need to resize the image, convert the image into a tensor representation, and preprocess the tensor. Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_img = Image.open('img/dog.jpeg')\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    # Make the image smaller \n",
    "    transforms.Resize((50, 50)), \n",
    "    # Convert it to a tensor\n",
    "    transforms.ToTensor(), \n",
    "    # Normalize each RGB channel of an image such that channel's max and min value fall between -1 and 1\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "])\n",
    "\n",
    "mod_img = trans(src_img)\n",
    "print(f\"Modified image dimension = {mod_img.shape}\")\n",
    "print(f\"Modified image max = {mod_img[0].max()}, min = {mod_img[0].min()}\")\n",
    "print(f\"Modified image standard deviation = {mod_img[0].std()}\")\n",
    "\n",
    "# Visualize and compare the transformation\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "axs[0].imshow(src_img)\n",
    "axs[0].set_title('Source')\n",
    "axs[1].imshow(mod_img.permute(1, 2, 0))\n",
    "axs[1].set_title('Modified')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba04a41",
   "metadata": {},
   "source": [
    "Why is transformation, particularly normalization, important? Short answer: it makes deep learning learns faster. However, the better answer can be found [here](https://becominghuman.ai/image-data-pre-processing-for-neural-networks-498289068258). To learn more about transformation, please refer to Torch Vision's transform [documentation](https://pytorch.org/vision/stable/transforms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971f17d",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "In the 2000s, we had linear/logistic regressions, decision trees (random forest), SVM, Bayesian networks, and etc for various machine learning tasks. By 2010s, all of them had been outperformed by neural network based deep learning architecture. This is especially true for natural language processing and computer vision tasks. Thus, all modern machine learning frameworks focus their implementation entirely on neural networks.\n",
    "\n",
    "<img alt=\"Neural Networks\" src=\"img/nn.png\" width=500>\n",
    "\n",
    "A neural network can be thought of as a chain of functions (they can be linear or non-linear). Let's look at a linear neural network that implements the following function.\n",
    "\n",
    "$$\n",
    "y = Mx + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8c6ea",
   "metadata": {},
   "source": [
    "### Linear Regression with Gradient Descent\n",
    "\n",
    "[Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w&ab_channel=3Blue1Brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self):\n",
    "        self.M = torch.randn(1, dtype=float, requires_grad=True)\n",
    "        self.b = torch.randn(1, dtype=float, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward carries out the computation to produce an output\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): input to the model\n",
    "        \"\"\"\n",
    "        out = self.M * x # Linear operation 1\n",
    "        out = out + self.b # Linear operation 2\n",
    "        return out\n",
    "    \n",
    "    def gradient_descent(self, lr=0.001):\n",
    "        \"\"\"Performs gradient descent to update model parameters.\n",
    "        \n",
    "        Args:\n",
    "            lr (float): learning rate determines how fast the network learns.\n",
    "        \"\"\"\n",
    "        self.M.data -= lr * self.M.grad.data\n",
    "        self.b.data -= lr * self.b.grad.data\n",
    "         # Reset the gradient after it's been used\n",
    "        self.M.grad.data.zero_()\n",
    "        self.b.grad.data.zero_() \n",
    "\n",
    "\n",
    "model = Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc670e",
   "metadata": {},
   "source": [
    "Let's generate some fake linear data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba10f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = (.1 * np.random.randn(100, 1))\n",
    "true_b = 1\n",
    "true_M = 2\n",
    "\n",
    "# Training data to teach the linear model\n",
    "x_train = np.random.rand(100, 1)\n",
    "y_train = true_M * x_train + true_b + epsilon\n",
    "\n",
    "# Convert numpy array into float tensor\n",
    "x_tensor = tensor(x_train, dtype=float)\n",
    "y_tensor = tensor(y_train, dtype=float)\n",
    "\n",
    "# At first the model will perform poorly because it hasn't been traiend yet.\n",
    "y_pred_tensor = model.forward(x_tensor)\n",
    "\n",
    "plt.plot(x_train, y_train, 'o', x_train, y_pred_tensor.detach().numpy(), 'o')\n",
    "plt.title('Line does not fit the data at all')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf172b",
   "metadata": {},
   "source": [
    "### Use Autograd to Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd0668",
   "metadata": {},
   "source": [
    "The objective of training is to reduce errors. We can plot the errors over our training iteration. What we expect to see is a sharp drop of error values as training goes on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an error function, also known as loss\n",
    "def squared_loss(y, y_predict):\n",
    "    loss_val = torch.sum((y - y_predict)**2)\n",
    "    return loss_val \n",
    "\n",
    "# Perform training\n",
    "losses = []\n",
    "for _ in range(500):\n",
    "    y_pred = model.forward(x_tensor) \n",
    "    loss = squared_loss(y_tensor, y_pred)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    model.gradient_descent(lr=0.001)\n",
    "    \n",
    "plt.plot(range(500), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15cf1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the model has learned.\n",
    "y_pred = model.forward(x_tensor)\n",
    "plt.plot(x_train, y_train, 'o', x_train, y_pred.detach().numpy(), 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the parameters, they should be very close to the true value\n",
    "print(model.M)\n",
    "print(model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b82bb",
   "metadata": {},
   "source": [
    "### Better Version of Linear Neural Network\n",
    "\n",
    "PyTorch provides neural network modules, loss functions and optimizer so we don't have to implement them ourselves. The following is identical to the implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09306bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1, dtype=float)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2660633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with the new Linear class.\n",
    "model = Linear()\n",
    "\n",
    "# Sum up the mean squared loss, so it's the same loss from what we have above.\n",
    "mean_squared_loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Optimizer performs gradient descent automatically. SGD stands for\n",
    "# stochastic gradient descent.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "losses = []\n",
    "for _ in range(500):\n",
    "    y_pred = model.forward(x_tensor)\n",
    "    loss = mean_squared_loss(y_tensor, y_pred)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "plt.plot(range(500), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the model has learned.\n",
    "y_pred = model.forward(x_tensor)\n",
    "plt.plot(x_train, y_train, 'o', x_train, y_pred.detach().numpy(), 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4769d640",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d164fa4",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b7de73",
   "metadata": {},
   "source": [
    "MNIST is a large dataset of handwritten digits. The objective is to classify and convert these handwritten digit images into machine understandable integer. First, let's download the dataset and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation\n",
    "transform = transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('data/', train=True, download=True, transform=transform)\n",
    "print(f\"training dataset has {len(train_dataset)} examples\")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST('data/', train=False, download=True, transform=transform)\n",
    "print(f\"test dataset has {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb41817",
   "metadata": {},
   "source": [
    "There are 2 sets of data. We will use the training dataset to train the model. The test dataset evaluates how well our model performs against new samples that the model hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5eb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(12):\n",
    "    x, y = train_dataset[i]\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.imshow(x.squeeze(0), cmap='gray')\n",
    "    plt.title(f\"Grouth Truth Y: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447c29b",
   "metadata": {},
   "source": [
    "### Batch Data\n",
    "\n",
    "Dataset need to be batched for performance reasons. Instead of feeding one image to the model at a time, we can feed `N` images at once. For convenience, we can use data loader to split data into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(f\"batch {i} has x shape {x.shape} and y shape {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3c5771",
   "metadata": {},
   "source": [
    "### Neural Network Modeling\n",
    "\n",
    "When it comes to modeling, there are 2 important deciding factors\n",
    "\n",
    "- Which model architecture to use for my dataset?\n",
    "- Which loss function to optimize for?\n",
    "\n",
    "The traditional architectural choice for image recognition is **convolutional neural networks**. We can compare the performance of a simple linear model to a convolutional model using the test dataset. Since we are performing classification, we will use **categorical cross entropy loss**.\n",
    "\n",
    "- [A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "- [Cross-Entropy Loss Function](https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58458f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We can stack many linear layers together to create a \"network\"\n",
    "        self.layer_1 = nn.Linear(28*28, 100)\n",
    "        self.layer_2 = nn.Linear(100, 100)\n",
    "        self.layer_3 = nn.Linear(100, 10)\n",
    "        # Use log softmax function convert numerical values into log(probabilities)\n",
    "        # Why log(probabilities)? It helps with computing the cross entropy loss later.\n",
    "        # We can easily convert it back to probabilities using exponential function.\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.flatten(x, 1) # Flatten the 28x28 image into a single dimensional vector\n",
    "        out = self.layer_1(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.layer_3(out)\n",
    "        out = self.log_softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearClassifier()\n",
    "x, y = next(iter(train_loader)) \n",
    "y_pred = model(x)\n",
    "probs = torch.exp(y_pred) # probability\n",
    "\n",
    "indices = list(map(lambda i: f\"sample {i}\" , range(len(x))))\n",
    "\n",
    "prob_df = pd.DataFrame(probs.detach().numpy(), index=indices)\n",
    "print(prob_df.round(2).head())\n",
    "\n",
    "print(\"\\nPredicted classes for each sample\\n\")\n",
    "\n",
    "class_df = prob_df.idxmax(axis=1)\n",
    "print(class_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a28a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # One layer is a stack of multiple units\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.25))\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.50))\n",
    "        self.layer_3 = nn.Linear(1600, 128)\n",
    "        self.layer_4 = nn.Linear(128, 10)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer_1(x) # Don't need to flatten the image because convolution works on 2D input.\n",
    "        out = self.layer_2(out)\n",
    "        out = torch.flatten(out, 1) # Flatten the convolutional outputs into a single dimensional vector\n",
    "        out = self.layer_3(out)\n",
    "        out = self.layer_4(out)\n",
    "        out = self.log_softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28fdc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvClassifier()\n",
    "x, y = next(iter(train_loader)) \n",
    "y_pred = model(x)\n",
    "probs = torch.exp(y_pred)\n",
    "\n",
    "indices = list(map(lambda i: f\"sample {i}\" , range(len(x))))\n",
    "\n",
    "prob_df = pd.DataFrame(probs.detach().numpy(), index=indices)\n",
    "print(prob_df.round(2).head())\n",
    "\n",
    "print(\"\\nPredicted classes for each sample\\n\")\n",
    "\n",
    "class_df = prob_df.idxmax(axis=1)\n",
    "print(class_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914082d",
   "metadata": {},
   "source": [
    "### Negative Log Loss\n",
    "\n",
    "When [negative log loss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) is applied on [log softmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html), the end result is a [cross entropy loss](https://pytorch.org/docs/1.9.1/generated/torch.nn.CrossEntropyLoss.html). `CrossEntropyLoss` is a more optimized implementation by combining the two calculations into one step. For the purpose of demonstration, I have separated them into two function calls.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearClassifier()\n",
    "x, y = next(iter(train_loader)) \n",
    "output = model(x)\n",
    "loss = functional.nll_loss(output, y)\n",
    "print(f\"Linear classifier loss value: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvClassifier()\n",
    "x, y = next(iter(train_loader)) \n",
    "output = model(x)\n",
    "loss = functional.nll_loss(output, y)\n",
    "print(f\"Convolutional classifier loss value: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a5c4a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a8f2d",
   "metadata": {},
   "source": [
    "For every training step, we perform the following logic\n",
    "\n",
    "- For every batch of training data\n",
    "  - 1. Forward pass input data to model and generate an output.\n",
    "  - 2. Compute loss using output and ground truth label.\n",
    "  - 3. Use auotgrad to compute gradients, also known as `backward()` or backpropagation.\n",
    "  - 4. Update the model parameters using optimizer.\n",
    "  - 5. Reset the gradients.\n",
    "- When training is completed on all batches of data, for every batch of test data\n",
    "  - 1. Disable gradient for the following logic i.e. `with torch.no_grad()`\n",
    "  - 2. Forward pass the input data to model and generate an output.\n",
    "  - 3. Compute loss using output and ground truth label.\n",
    "  - 4. Calculate accuracy of detection.\n",
    "\n",
    "Then we repeat the training step `N` times known as `epochs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8eac3d",
   "metadata": {},
   "source": [
    "> If you have GPU, PyTorch will run computation on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, train_loader, log_interval=100):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_i, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        loss = functional.nll_loss(output, y)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if batch_i % log_interval == 0:\n",
    "            print((\n",
    "                f\"Training [epoch {epoch:02d}][{batch_i:04d}/{len(train_loader):04d}]\"\n",
    "                f\"\\tLoss {loss.item():.6f}\"\n",
    "            ))\n",
    "    return np.average(losses)\n",
    "\n",
    "\n",
    "def test(epoch, model, test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    num_correct_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_i, (x, y) in enumerate(test_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = functional.nll_loss(output, y)\n",
    "            losses.append(loss.item()) \n",
    "            # Convert probabilities into prediction, pick the most probable class.\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # Convert target into a one-hot representation, then compare match rate.\n",
    "            num_correct_preds += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print((\n",
    "        f\"Testing [epoch {epoch:02d}] Average Loss: {np.average(losses)}\"\n",
    "        f\"\\tAccuracy {100.0 * num_correct_preds / len(test_loader.dataset):0.2f}%\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c034cc6",
   "metadata": {},
   "source": [
    "Now define our models and optimizers. We need to send our model to device, i.e. send it to a CPU or GPU. The learning rate for optimizers is a **hyperparameter**. We need to run experimentation to find the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f96be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearClassifier().to(device)\n",
    "linear_optim = optim.SGD(linear_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d41f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = ConvClassifier().to(device)\n",
    "conv_optim = optim.SGD(conv_model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a19edc",
   "metadata": {},
   "source": [
    "Before training, our model should have a 10% accuracy which means it is randomly guessing a number from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768cc702",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(0, linear_model, test_loader)\n",
    "test(0, conv_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3da39",
   "metadata": {},
   "source": [
    "### Train Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 # We will train it for 10 epochs, 10 training/testing steps.\n",
    "log_interval = 375 # We log once every 1000 batches have processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ae059",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    epoch_losses.append(train(epoch, linear_model, linear_optim, train_loader,\n",
    "                              log_interval=log_interval))\n",
    "    test(epoch, linear_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c605adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, num_epochs+1), epoch_losses, '-o')\n",
    "plt.title('Loss over Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037e851",
   "metadata": {},
   "source": [
    "### Train Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ea8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    epoch_losses.append(train(epoch, conv_model, conv_optim, train_loader,\n",
    "                              log_interval=log_interval))\n",
    "    test(epoch, conv_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9102a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, num_epochs+1), epoch_losses, '-o')\n",
    "plt.title('Loss over Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ec0f5",
   "metadata": {},
   "source": [
    "We can play around with different\n",
    "\n",
    "- model architecture\n",
    "- batch size e.g. 8, 16, 32, 64\n",
    "- [optimizer](https://pytorch.org/docs/stable/optim.html) e.g. SGD, Adam, Adagrad \n",
    "- learning rate e.g. 0.001, 0.01, 0.1, 1.0\n",
    "\n",
    "to achieve a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb470541",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8489e76",
   "metadata": {},
   "source": [
    "Let's visualize what is the model predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_loader))\n",
    "x, y = x.to(device), y.to(device)\n",
    "output = conv_model(x)\n",
    "pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 30))\n",
    "for i in range(len(x)):\n",
    "    plt.subplot(8, 4, i+1)\n",
    "    plt.imshow(x[i].detach().cpu().numpy().squeeze(0), cmap='gray')\n",
    "    plt.title(f\"Predicted: {pred[i].item()}\")\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f3476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
